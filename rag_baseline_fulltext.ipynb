{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LitSearch RAG Baseline using LangChain with Full Corpus\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Using regular tqdm instead of tqdm.notebook\n",
    "from typing import List, Dict, Any, Union, Optional\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Metrics\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Import for datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Replace with your API key\"  # Replace with your API key\n",
    "# You can also set it as: os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Loading the Datasets\n",
    "\n",
    "# Function to load user dataset (handles both JSON and CSV)\n",
    "def load_user_dataset(file_path: str, num_samples: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Load user dataset from JSON or CSV file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to dataset file (JSON or CSV)\n",
    "        num_samples: Number of samples to use (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing the dataset\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.json'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dataset = data.get('data', data)  # Handle both {'data': [...]} and direct list format\n",
    "    elif file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataset = df.to_dict(orient='records')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use JSON or CSV.\")\n",
    "    \n",
    "    # Sample if requested\n",
    "    if num_samples and num_samples < len(dataset):\n",
    "        import random\n",
    "        random.seed(42)  # For reproducibility\n",
    "        dataset = random.sample(dataset, num_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(dataset)} samples from {file_path}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 samples from /Users/himansh/Desktop/ANLP/litsearch/litsearch_rag_dataset_fullpaper_100.json\n",
      "Loading LitSearch corpus dataset...\n",
      "Loaded 64183 papers from LitSearch corpus\n",
      "Corpus data fields: ['corpusid', 'title', 'abstract', 'citations', 'full_paper']\n",
      "Using 'corpusid' as paper ID field\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64183/64183 [00:11<00:00, 5768.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created corpus dictionary with 64183 papers\n",
      "\n",
      "Sample from user dataset:\n",
      "{'original_query': 'Could you suggest research on detecting common errors like additions and omissions in machine translation?', 'paper_title': 'Detecting Over-and Undertranslations with Contrastive Conditioning', 'paper_abstract': 'Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.', 'used_full_text': True, 'paper_id': 247223093, 'conceptual_question': '\"How does the method of contrastive conditioning help in detecting over- and undertranslations in neural machine translation, and how does it compare to supervised approaches?\",', 'ground_truth_answer': '\"The method of contrastive conditioning proposed in the paper \\'Detecting Over-and Undertranslations with Contrastive Conditioning\\' enables the detection of addition (overtranslation) and omission (undertranslation) errors without the need for reference translations. This method operates by comparing the likelihood of a complete sentence under a translation model with the likelihood of its parts. The technique leverages off-the-shelf neural machine translation (NMT) models to identify untranslated words in the source and superfluous words in the translation. By constructing partial source and target sequences (using dependency parse trees to define potential error spans), the system checks whether omitting specific parts from the sentence leads to a higher probability score than the full sequence, indicating a possible omission. Conversely, addition errors are detected by applying a similar procedure in reverse translation direction.\\\\n\\\\nIn evaluation, the contrastive conditioning method successfully detected omission errors with higher accuracy than a supervised quality estimation (QE) baseline, which relied on synthetic data for training. While the accuracy in detecting additions was lower, the method still offered a useful comparison to human-annotated and synthetically-generated datasets. Unlike supervised methods that require custom models trained on large corpora of labeled data, contrastive conditioning can be applied directly using existing NMT architectures, making it a valuable tool where post-editing by human translators is involved. However, the method\\'s detection precision is notably higher for omissions than for additions due to syntactical variances often leading to false positives in addition detection. Despite these challenges, the method holds promise for effective implementation in practical machine translation tasks.\" } ```'}\n",
      "\n",
      "Sample from corpus:\n",
      "Paper ID: 252715594\n",
      "Title: PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n",
      "Abstract: We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited qu...\n",
      "Full text length: 65298 characters\n",
      "Full text sample: PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n",
      "\n",
      "\n",
      "Ruben Villegas \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University Col...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to load LitSearch corpus from Hugging Face\n",
    "def load_litsearch_corpus():\n",
    "    \"\"\"\n",
    "    Load the LitSearch corpus from Hugging Face\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping paper IDs to paper content\n",
    "    \"\"\"\n",
    "    print(\"Loading LitSearch corpus dataset...\")\n",
    "    try:\n",
    "        # Load the corpus\n",
    "        corpus_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "        print(f\"Loaded {len(corpus_data)} papers from LitSearch corpus\")\n",
    "        \n",
    "        # Print a sample to understand the structure\n",
    "        if len(corpus_data) > 0:\n",
    "            print(\"Corpus data fields:\", list(corpus_data[0].keys()))\n",
    "        \n",
    "        # Create a dictionary mapping paper IDs to paper content\n",
    "        corpus_dict = {}\n",
    "        paper_id_field = None\n",
    "        \n",
    "        # Determine the field containing paper IDs\n",
    "        if len(corpus_data) > 0:\n",
    "            sample = corpus_data[0]\n",
    "            if \"paper_id\" in sample:\n",
    "                paper_id_field = \"paper_id\"\n",
    "            elif \"doc_id\" in sample:\n",
    "                paper_id_field = \"doc_id\"\n",
    "            elif \"corpusid\" in sample:\n",
    "                paper_id_field = \"corpusid\"\n",
    "            else:\n",
    "                # Find a field that looks like an ID\n",
    "                for key in sample.keys():\n",
    "                    if \"id\" in key.lower():\n",
    "                        paper_id_field = key\n",
    "                        break\n",
    "        \n",
    "        if not paper_id_field:\n",
    "            raise ValueError(\"Could not determine paper ID field in corpus dataset\")\n",
    "        \n",
    "        print(f\"Using '{paper_id_field}' as paper ID field\")\n",
    "        \n",
    "        # Create the mapping\n",
    "        for item in tqdm(corpus_data):\n",
    "            paper_id = item.get(paper_id_field)\n",
    "            if paper_id:\n",
    "                corpus_dict[paper_id] = {\n",
    "                    'title': item.get('title', ''),\n",
    "                    'abstract': item.get('abstract', ''),\n",
    "                    'full_text': item.get('full_paper', '')  # Get full paper text if available\n",
    "                }\n",
    "        \n",
    "        print(f\"Created corpus dictionary with {len(corpus_dict)} papers\")\n",
    "        return corpus_dict, paper_id_field\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LitSearch corpus: {e}\")\n",
    "        print(\"Falling back to using only title and abstract from user dataset\")\n",
    "        return {}, None\n",
    "\n",
    "# Load your dataset\n",
    "user_dataset_path = \"/Users/himansh/Desktop/ANLP/litsearch/litsearch_rag_dataset_fullpaper_100.json\"  # Replace with your path\n",
    "user_dataset = load_user_dataset(user_dataset_path, num_samples=50)  # Adjust sample size as needed\n",
    "\n",
    "# Load LitSearch corpus\n",
    "corpus_dict, paper_id_field = load_litsearch_corpus()\n",
    "\n",
    "# Display a sample from user dataset\n",
    "print(\"\\nSample from user dataset:\")\n",
    "print(user_dataset[0])\n",
    "\n",
    "# Display a sample from corpus (if available)\n",
    "if corpus_dict:\n",
    "    print(\"\\nSample from corpus:\")\n",
    "    sample_id = next(iter(corpus_dict))\n",
    "    print(f\"Paper ID: {sample_id}\")\n",
    "    print(f\"Title: {corpus_dict[sample_id]['title']}\")\n",
    "    abstract = corpus_dict[sample_id]['abstract']\n",
    "    print(f\"Abstract: {abstract[:200]}...\")  # Show just the beginning\n",
    "    full_text = corpus_dict[sample_id].get('full_text', '')\n",
    "    if full_text:\n",
    "        print(f\"Full text length: {len(full_text)} characters\")\n",
    "        print(f\"Full text sample: {full_text[:200]}...\")  # Show just the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 LangChain documents\n",
      "  - 50 documents include full text\n",
      "  - 0 documents include abstract only\n"
     ]
    }
   ],
   "source": [
    "## 2. Preparing Documents for LangChain\n",
    "\n",
    "def prepare_documents(user_dataset, corpus_dict):\n",
    "    \"\"\"\n",
    "    Convert dataset to LangChain documents format using corpus when available\n",
    "    \"\"\"\n",
    "    langchain_docs = []\n",
    "    \n",
    "    # Create a set of all paper IDs in the user dataset\n",
    "    paper_ids_set = set()\n",
    "    for item in user_dataset:\n",
    "        paper_id = item.get('paper_id')\n",
    "        if paper_id:\n",
    "            paper_ids_set.add(paper_id)\n",
    "    \n",
    "    # Track how many papers have full text\n",
    "    papers_with_full_text = 0\n",
    "    papers_with_abstract_only = 0\n",
    "    \n",
    "    # Create documents for each paper\n",
    "    for paper_id in paper_ids_set:\n",
    "        # Look for the paper in the corpus\n",
    "        if paper_id in corpus_dict and corpus_dict[paper_id]['full_text']:\n",
    "            # Use full text from corpus\n",
    "            paper = corpus_dict[paper_id]\n",
    "            content = f\"Title: {paper['title']}\\n\\nAbstract: {paper['abstract']}\\n\\nFull Text: {paper['full_text']}\"\n",
    "            papers_with_full_text += 1\n",
    "        else:\n",
    "            # Fallback to using title and abstract from the user dataset\n",
    "            # Find this paper in the user dataset\n",
    "            paper_data = next((item for item in user_dataset if item.get('paper_id') == paper_id), None)\n",
    "            if paper_data:\n",
    "                title = paper_data.get('paper_title', '')\n",
    "                abstract = paper_data.get('paper_abstract', '')\n",
    "                content = f\"Title: {title}\\n\\nAbstract: {abstract}\"\n",
    "                papers_with_abstract_only += 1\n",
    "            else:\n",
    "                # Skip if we can't find any information about this paper\n",
    "                continue\n",
    "        \n",
    "        # Create LangChain document\n",
    "        langchain_docs.append(\n",
    "            Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'paper_id': paper_id,\n",
    "                    'source': 'LitSearch'\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(f\"Created {len(langchain_docs)} LangChain documents\")\n",
    "    print(f\"  - {papers_with_full_text} documents include full text\")\n",
    "    print(f\"  - {papers_with_abstract_only} documents include abstract only\")\n",
    "    return langchain_docs\n",
    "\n",
    "# Create LangChain documents\n",
    "documents = prepare_documents(user_dataset, corpus_dict)\n",
    "\n",
    "# Create a lookup dictionary for ground truth papers\n",
    "ground_truth_lookup = {}\n",
    "for item in user_dataset:\n",
    "    question = item.get('conceptual_question')\n",
    "    if question:\n",
    "        ground_truth_lookup[question] = {\n",
    "            'paper_id': item.get('paper_id'),\n",
    "            'answer': item.get('ground_truth_answer')\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4619 chunks from 50 documents\n"
     ]
    }
   ],
   "source": [
    "## 3. Create Chunker for Longer Documents\n",
    "\n",
    "# Text splitter for longer documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Split documents if they're long\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(split_docs)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Create Retrievers\n",
    "\n",
    "# 1. BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = 20  # Top-k documents to retrieve\n",
    "\n",
    "# 2. Dense Retriever with FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# 3. Ensemble Retriever (combines BM25 and Dense)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, dense_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Create RAG Chains with LangChain\n",
    "\n",
    "# Define a more specific prompt for scientific questions\n",
    "prompt_template = \"\"\"You are a helpful scientific assistant. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6z/nz9cvz6s3vg6yvj80zkc7hmm0000gn/T/ipykernel_1837/3920281089.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", base_url=\"https://cmu.litellm.ai\")\n"
     ]
    }
   ],
   "source": [
    "# Create LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", base_url=\"https://cmu.litellm.ai\")\n",
    "\n",
    "# Create RetrievalQA chains for each retriever\n",
    "def create_qa_chain(retriever, llm=llm, prompt=PROMPT):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "# Create the chains\n",
    "bm25_chain = create_qa_chain(bm25_retriever)\n",
    "dense_chain = create_qa_chain(dense_retriever)\n",
    "ensemble_chain = create_qa_chain(ensemble_retriever)\n",
    "\n",
    "# Zero-shot chain (no retrieval)\n",
    "zero_shot_template = \"\"\"You are a helpful scientific assistant with expertise in machine learning, AI, and computer science research. Answer the following question based on your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "ZERO_SHOT_PROMPT = PromptTemplate(\n",
    "    template=zero_shot_template,\n",
    "    input_variables=[\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Run Experiments\n",
    "\n",
    "def evaluate_retrieval(retrieved_docs, ground_truth_paper_id):\n",
    "    \"\"\"Evaluate retrieval performance\"\"\"\n",
    "    # Extract paper IDs from chunks\n",
    "    retrieved_paper_ids = set()\n",
    "    for doc in retrieved_docs:\n",
    "        paper_id = doc.metadata.get('paper_id')\n",
    "        if paper_id:\n",
    "            retrieved_paper_ids.add(paper_id)\n",
    "    \n",
    "    # Convert to list for indexing\n",
    "    retrieved_paper_ids_list = list(retrieved_paper_ids)\n",
    "    \n",
    "    # Check if ground truth is in retrieved docs\n",
    "    if ground_truth_paper_id in retrieved_paper_ids_list:\n",
    "        rank = retrieved_paper_ids_list.index(ground_truth_paper_id)\n",
    "        mrr = 1.0 / (rank + 1)\n",
    "        found = True\n",
    "    else:\n",
    "        mrr = 0.0\n",
    "        found = False\n",
    "    \n",
    "    return {\n",
    "        \"found\": found,\n",
    "        \"mrr\": mrr,\n",
    "        \"retrieved_paper_ids\": retrieved_paper_ids_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required package\n",
    "# pip install bert-score\n",
    "\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "def evaluate_answer(generated, ground_truth):\n",
    "    \"\"\"Evaluate answer quality using both ROUGE and BERTScore with SCIBert\"\"\"\n",
    "    # ROUGE scores\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, generated)\n",
    "    \n",
    "    # BERTScore with SCIBert\n",
    "    P, R, F1 = bert_score(\n",
    "        [generated], \n",
    "        [ground_truth], \n",
    "        model_type=\"allenai/scibert_scivocab_uncased\",  # Use SCIBert\n",
    "        lang=\"en\",\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "        \"bertscore_precision\": P.item(),\n",
    "        \"bertscore_recall\": R.item(),\n",
    "        \"bertscore_f1\": F1.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(chain, question, ground_truth_info):\n",
    "    \"\"\"Run experiment with a specific chain\"\"\"\n",
    "    # Track token usage\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain({\"query\": question})\n",
    "    \n",
    "    # Extract answer and retrieved documents\n",
    "    answer = result.get(\"result\", \"\")\n",
    "    source_docs = result.get(\"source_documents\", [])\n",
    "    \n",
    "    # Evaluate retrieval if we have source documents\n",
    "    retrieval_metrics = {}\n",
    "    if source_docs:\n",
    "        retrieval_metrics = evaluate_retrieval(source_docs, ground_truth_info['paper_id'])\n",
    "    \n",
    "    # Evaluate answer\n",
    "    answer_metrics = evaluate_answer(answer, ground_truth_info['answer'])\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieval_metrics\": retrieval_metrics,\n",
    "        \"answer_metrics\": answer_metrics,\n",
    "        \"token_usage\": {\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"cost\": cb.total_cost\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot(llm, question, ground_truth_info):\n",
    "    \"\"\"Run zero-shot experiment\"\"\"\n",
    "    # Track token usage\n",
    "    with get_openai_callback() as cb:\n",
    "        # Format the input properly for a ChatOpenAI model\n",
    "        formatted_prompt = ZERO_SHOT_PROMPT.format(question=question)\n",
    "        \n",
    "        # Use the predict method directly instead of generate\n",
    "        answer = llm.predict(formatted_prompt)\n",
    "    \n",
    "    # Evaluate answer\n",
    "    answer_metrics = evaluate_answer(answer, ground_truth_info['answer'])\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"answer_metrics\": answer_metrics,\n",
    "        \"token_usage\": {\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"cost\": cb.total_cost\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Run All Baselines\n",
    "\n",
    "def run_all_baselines(dataset, num_samples=None):\n",
    "    \"\"\"Run all baseline methods\"\"\"\n",
    "    if num_samples:\n",
    "        samples = dataset[:num_samples]\n",
    "    else:\n",
    "        samples = dataset\n",
    "    \n",
    "    results = {\n",
    "        \"bm25\": [],\n",
    "        \"dense\": [],\n",
    "        \"ensemble\": [],\n",
    "        \"zero_shot\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Running baselines on {len(samples)} samples...\")\n",
    "    for i, sample in enumerate(tqdm(samples)):\n",
    "        question = sample.get('conceptual_question')\n",
    "        # Skip if question is missing\n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        # Get ground truth info\n",
    "        ground_truth_info = ground_truth_lookup.get(question, {})\n",
    "        if not ground_truth_info:\n",
    "            print(f\"Warning: No ground truth found for question: {question[:50]}...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing question {i+1}/{len(samples)}: {question[:100]}...\")\n",
    "        \n",
    "        # Run BM25 experiment\n",
    "        print(\"Running BM25 + LLM...\")\n",
    "        bm25_result = run_experiment(bm25_chain, question, ground_truth_info)\n",
    "        results[\"bm25\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": bm25_result[\"answer\"],\n",
    "            \"retrieval_metrics\": bm25_result[\"retrieval_metrics\"],\n",
    "            \"answer_metrics\": bm25_result[\"answer_metrics\"],\n",
    "            \"token_usage\": bm25_result[\"token_usage\"]\n",
    "        })\n",
    "        \n",
    "        # Run Dense experiment\n",
    "        print(\"Running Dense + LLM...\")\n",
    "        dense_result = run_experiment(dense_chain, question, ground_truth_info)\n",
    "        results[\"dense\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": dense_result[\"answer\"],\n",
    "            \"retrieval_metrics\": dense_result[\"retrieval_metrics\"],\n",
    "            \"answer_metrics\": dense_result[\"answer_metrics\"],\n",
    "            \"token_usage\": dense_result[\"token_usage\"]\n",
    "        })\n",
    "        \n",
    "        # Run Ensemble experiment\n",
    "        print(\"Running Ensemble + LLM...\")\n",
    "        ensemble_result = run_experiment(ensemble_chain, question, ground_truth_info)\n",
    "        results[\"ensemble\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": ensemble_result[\"answer\"],\n",
    "            \"retrieval_metrics\": ensemble_result[\"retrieval_metrics\"],\n",
    "            \"answer_metrics\": ensemble_result[\"answer_metrics\"],\n",
    "            \"token_usage\": ensemble_result[\"token_usage\"]\n",
    "        })\n",
    "        \n",
    "        # Run Zero-shot experiment\n",
    "        print(\"Running Zero-shot...\")\n",
    "        zero_shot_result = run_zero_shot(llm, question, ground_truth_info)\n",
    "        results[\"zero_shot\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": zero_shot_result[\"answer\"],\n",
    "            \"answer_metrics\": zero_shot_result[\"answer_metrics\"],\n",
    "            \"token_usage\": zero_shot_result[\"token_usage\"]\n",
    "        })\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    calculate_summary_metrics(results)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_summary_metrics(results):\n",
    "    \"\"\"Calculate summary metrics for all methods\"\"\"\n",
    "    methods = [\"bm25\", \"dense\", \"ensemble\", \"zero_shot\"]\n",
    "    summary = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        method_results = results[method]\n",
    "        \n",
    "        # Skip if no results\n",
    "        if not method_results:\n",
    "            continue\n",
    "            \n",
    "        method_summary = {\n",
    "            \"answer_metrics\": {\n",
    "                \"rouge1\": 0.0,\n",
    "                \"rouge2\": 0.0,\n",
    "                \"rougeL\": 0.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add retrieval metrics for retrieval-based methods\n",
    "        if method != \"zero_shot\":\n",
    "            method_summary[\"retrieval_metrics\"] = {\n",
    "                \"found_rate\": 0.0,\n",
    "                \"mrr\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Calculate answer metrics\n",
    "        for result in method_results:\n",
    "            for metric in method_summary[\"answer_metrics\"]:\n",
    "                method_summary[\"answer_metrics\"][metric] += result[\"answer_metrics\"][metric]\n",
    "        \n",
    "        # Calculate retrieval metrics\n",
    "        if method != \"zero_shot\":\n",
    "            for result in method_results:\n",
    "                method_summary[\"retrieval_metrics\"][\"found_rate\"] += 1 if result[\"retrieval_metrics\"].get(\"found\", False) else 0\n",
    "                method_summary[\"retrieval_metrics\"][\"mrr\"] += result[\"retrieval_metrics\"].get(\"mrr\", 0.0)\n",
    "        \n",
    "        # Calculate averages\n",
    "        n = len(method_results)\n",
    "        for metric in method_summary[\"answer_metrics\"]:\n",
    "            method_summary[\"answer_metrics\"][metric] /= n\n",
    "        \n",
    "        if method != \"zero_shot\":\n",
    "            for metric in method_summary[\"retrieval_metrics\"]:\n",
    "                method_summary[\"retrieval_metrics\"][metric] /= n\n",
    "        \n",
    "        summary[method] = method_summary\n",
    "    \n",
    "    results[\"summary\"] = summary\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baselines on 50 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 1/50: \"How does the method of contrastive conditioning help in detecting over- and undertranslations in ne...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:24<20:00, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 2/50: \"How does the CoFi method's approach to structured pruning compare to traditional distillation metho...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:44<17:39, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 3/50: \"How does the ConvGQR model improve query reformulation in conversational search, and what are its k...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:09<18:06, 23.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 4/50: \"How does the proposed neural architecture for dialectal Arabic segmentation handle the unique chall...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:24<15:28, 20.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 5/50: \"How does the SALMON approach facilitate AI alignment with minimal human supervision, and how does i...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:45<15:09, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 6/50: \"What is the COVID-19-Stance dataset, and how does it contribute to research in stance detection reg...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:07<15:17, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 7/50: \"How does Pattern-Exploiting Training (PET) improve performance in few-shot text classification and ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [02:29<15:13, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 8/50: \"What methodologies and evaluation metrics were employed in the Semantic Role Labeling subtask of Se...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [02:57<16:27, 23.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 9/50: \"How does the ALIGNSCORE metric utilize a mean-max aggregation approach to evaluate factual consiste...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [03:21<16:13, 23.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 10/50: \"What are the key findings of the paper 'Distill or Annotate? Cost-Efficient Fine-Tuning of Compact ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [03:40<14:49, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 11/50: \"How does the Neural Path Hunter (NPH) method reduce hallucinations in dialogue systems using knowle...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [03:58<13:37, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 12/50: \"How does the MultiTACRED dataset enhance multilingual relation extraction, and what are the main ch...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [04:26<14:29, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 13/50: \"How does the Basis-Aware Threshold (BAT) sampling method proposed in the paper address the softmax ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [04:44<13:12, 21.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 14/50: \"What methods and datasets were used in the iSarcasmEval shared task for detecting intended sarcasm ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [05:08<13:20, 22.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 15/50: \"What are the unique features and challenges of the MAD-TSC dataset in performing target-dependent s...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [05:33<13:26, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 16/50: \"How do neurons in a two-layer ReLU network with small initialization achieve alignment during train...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [05:59<13:31, 23.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 17/50: \"How does the CRONQUESTIONS dataset facilitate progress in Temporal Knowledge Graph Question Answeri...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [06:21<12:57, 23.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 18/50: \"What are the key contributions of the XLM-R model in multilingual representation learning and how d...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [06:59<14:50, 27.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 19/50: \"How does the CALOR corpus facilitate semantic frame parsing for information extraction, and what ar...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [07:23<13:45, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 20/50: \"How does the DGSlow method utilize multi-objective optimization to create effective adversarial att...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [07:45<12:34, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 21/50: \"How does the DCLR framework address the issues of false negatives and sampling bias in unsupervised...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [08:07<11:42, 24.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 22/50: \"How does the scalability of pre-trained language models impact the efficacy of prompt tuning compar...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [08:36<12:02, 25.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 23/50: \"How does the PAED model address the challenges of zero-shot persona attribute extraction in dialogu...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [09:07<12:17, 27.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 24/50: \"How can gaze fixation patterns reveal a reader's native language when reading in English, and what ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [09:27<10:51, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 25/50: \"How can the Universal Joy dataset be used to improve emotion classification across multiple languag...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [09:51<10:18, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 26/50: \"How does the integration of concept co-occurrence graphs and pretrained language models facilitate ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [10:35<12:10, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 27/50: \"How does the QuestEval metric improve the evaluation of summarization accuracy and align with human...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [10:59<10:59, 28.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 28/50: \"How does linear decomposition help interpret Transformer models, particularly in identifying source...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [11:24<10:07, 27.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 29/50: \"How does LayoutMask improve text-layout interactions in multi-modal document pre-training, and what...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [11:44<08:50, 25.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 30/50: \"How does the KILT benchmark facilitate research on knowledge-intensive language tasks, and what are...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [12:06<08:03, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 31/50: \"How do language models compare to humans in interpreting non-literal language phenomena such as iro...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [12:43<08:56, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 32/50: \"How does the in-sample curriculum learning (ICL) strategy improve the performance of natural langua...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [13:31<10:11, 34.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 33/50: \"What are the main challenges and solutions proposed for improving annotation quality in cross-lingu...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [13:58<09:04, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 34/50: \"How does the order of in-context examples affect the performance of language models, and what metho...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [14:21<07:49, 29.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 35/50: \"How does the multi-dialect neural machine translation model for Japanese improve translation accura...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [14:49<07:11, 28.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 36/50: \"How does incorporating distribution-balanced loss functions improve multi-label text classification...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [15:24<07:10, 30.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 37/50: \"How does the classifier-based parser described in the paper handle the transformation and detransfo...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [15:49<06:17, 29.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 38/50: \"What challenges do large language models face in being evaluated accurately for open-domain questio...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [16:21<05:59, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 39/50: \"How does the ULTRA approach enable zero-shot and fine-tuned knowledge graph reasoning with unseen e...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [16:48<05:19, 29.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 40/50: \"How do factual and contextual error types differ in language models when generating descriptions fo...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [17:12<04:35, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 41/50: \"How does the SDP-LSTM model utilize shortest dependency paths and LSTM networks to improve relation...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [18:09<05:26, 36.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 42/50: \"How does the Geometric Transform Attention (GTA) mechanism improve the handling of positional encod...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [19:07<05:41, 42.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 43/50: \"What are the main challenges faced by open-domain question answering models when dealing with quest...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [19:32<04:23, 37.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 44/50: \"What is the Target-Stance Extraction (TSE) task proposed in stance detection, and how does it diffe...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [19:55<03:19, 33.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 45/50: \"How does the human-in-the-loop evaluation framework enhance the detection of novel misinformation c...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [20:22<02:36, 31.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 46/50: \"How does the mT5 model achieve state-of-the-art performance in multilingual NLP tasks, and what are...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [20:55<02:06, 31.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 47/50: \"How does the unified segmentation model for Arabic dialects discussed in 'Learning from Relatives: ...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [21:19<01:28, 29.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 48/50: \"How does the multi-task based approach to annotator disagreement, as explored in the paper 'Dealing...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [21:47<00:57, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 49/50: \"How does instruction tuning affect the scalability and performance of Mixture-of-Experts models com...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [22:11<00:27, 27.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 50/50: \"How does the PuMer framework improve inference speed and memory efficiency in large-scale Vision La...\n",
      "Running BM25 + LLM...\n",
      "Running Dense + LLM...\n",
      "Running Ensemble + LLM...\n",
      "Running Zero-shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [22:32<00:00, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "===============\n",
      "\n",
      "BM25:\n",
      "  Retrieval Success Rate: 0.9800\n",
      "  MRR: 0.9367\n",
      "  ROUGE-1: 0.4489\n",
      "  ROUGE-2: 0.1865\n",
      "  ROUGE-L: 0.2547\n",
      "\n",
      "DENSE:\n",
      "  Retrieval Success Rate: 1.0000\n",
      "  MRR: 0.9600\n",
      "  ROUGE-1: 0.4166\n",
      "  ROUGE-2: 0.1703\n",
      "  ROUGE-L: 0.2345\n",
      "\n",
      "ENSEMBLE:\n",
      "  Retrieval Success Rate: 1.0000\n",
      "  MRR: 0.9133\n",
      "  ROUGE-1: 0.4841\n",
      "  ROUGE-2: 0.2012\n",
      "  ROUGE-L: 0.2695\n",
      "\n",
      "ZERO_SHOT:\n",
      "  ROUGE-1: 0.4814\n",
      "  ROUGE-2: 0.1398\n",
      "  ROUGE-L: 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run all baselines with a small sample first (adjust as needed)\n",
    "num_samples = 50  # Start small for testing\n",
    "results = run_all_baselines(user_dataset, num_samples=num_samples)\n",
    "\n",
    "## 8. Analyze Results\n",
    "\n",
    "def print_summary(results):\n",
    "    \"\"\"Print summary of results\"\"\"\n",
    "    summary = results[\"summary\"]\n",
    "    \n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(\"===============\")\n",
    "    \n",
    "    methods = [\"bm25\", \"dense\", \"ensemble\", \"zero_shot\"]\n",
    "    for method in methods:\n",
    "        if method not in summary:\n",
    "            continue\n",
    "            \n",
    "        method_summary = summary[method]\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        \n",
    "        # Print retrieval metrics\n",
    "        if \"retrieval_metrics\" in method_summary:\n",
    "            print(f\"  Retrieval Success Rate: {method_summary['retrieval_metrics']['found_rate']:.4f}\")\n",
    "            print(f\"  MRR: {method_summary['retrieval_metrics']['mrr']:.4f}\")\n",
    "        \n",
    "        # Print answer metrics\n",
    "        print(f\"  ROUGE-1: {method_summary['answer_metrics']['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {method_summary['answer_metrics']['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {method_summary['answer_metrics']['rougeL']:.4f}\")\n",
    "\n",
    "# Print summary\n",
    "print_summary(results) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 9. Visualize Results\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"Plot comparison of different methods\"\"\"\n",
    "    summary = results[\"summary\"]\n",
    "    methods = [\"bm25\", \"dense\", \"ensemble\", \"zero_shot\"]\n",
    "    \n",
    "    # Filter methods that have results\n",
    "    methods = [method for method in methods if method in summary]\n",
    "    \n",
    "    # Get ROUGE-L scores\n",
    "    rouge_l_scores = [summary[method][\"answer_metrics\"][\"rougeL\"] for method in methods]\n",
    "    \n",
    "    # Get retrieval success rates\n",
    "    retrieval_rates = []\n",
    "    for method in methods:\n",
    "        if \"retrieval_metrics\" in summary[method]:\n",
    "            retrieval_rates.append(summary[method][\"retrieval_metrics\"][\"found_rate\"])\n",
    "        else:\n",
    "            retrieval_rates.append(0)  # Zero-shot has no retrieval\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot ROUGE-L scores\n",
    "    ax1.bar(methods, rouge_l_scores)\n",
    "    ax1.set_title('ROUGE-L Score by Method')\n",
    "    ax1.set_ylabel('ROUGE-L Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot retrieval success rates\n",
    "    ax2.bar(methods, retrieval_rates)\n",
    "    ax2.set_title('Retrieval Success Rate by Method')\n",
    "    ax2.set_ylabel('Success Rate')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to langchain_rag_results.json\n"
     ]
    }
   ],
   "source": [
    "## 10. Save Results\n",
    "\n",
    "def save_results(results, output_file=\"langchain_rag_results.json\"):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    # Convert numpy values to Python types for JSON serialization\n",
    "    def convert_for_json(obj):\n",
    "        if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_for_json(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_for_json(value) for key, value in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    converted_results = convert_for_json(results)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(converted_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Save results\n",
    "save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litsearch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
