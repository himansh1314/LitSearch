{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LitSearch RAG with Context-Enhanced Scientific Paper Chunking\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Union, Optional\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Hugging Face datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# BERTScore\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# ROUGE metrics\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Set OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Replace with your API key\"  # Replace with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "## 1. Loading the Datasets\n",
    "\n",
    "# Function to load user dataset (handles both JSON and CSV)\n",
    "def load_user_dataset(file_path: str, num_samples: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Load user dataset from JSON or CSV file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to dataset file (JSON or CSV)\n",
    "        num_samples: Number of samples to use (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing the dataset\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.json'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dataset = data.get('data', data)  # Handle both {'data': [...]} and direct list format\n",
    "    elif file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataset = df.to_dict(orient='records')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use JSON or CSV.\")\n",
    "    \n",
    "    # Sample if requested\n",
    "    if num_samples and num_samples < len(dataset):\n",
    "        import random\n",
    "        random.seed(42)  # For reproducibility\n",
    "        dataset = random.sample(dataset, num_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(dataset)} samples from {file_path}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 samples from /Users/himansh/Desktop/ANLP/litsearch/litsearch_rag_dataset_fullpaper_500.json\n",
      "Loading LitSearch corpus dataset...\n",
      "Loaded 64183 papers from LitSearch corpus\n",
      "Corpus data fields: ['corpusid', 'title', 'abstract', 'citations', 'full_paper']\n",
      "Using 'corpusid' as paper ID field\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64183/64183 [00:10<00:00, 6016.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created corpus dictionary with 64183 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to load LitSearch corpus from Hugging Face\n",
    "def load_litsearch_corpus():\n",
    "    \"\"\"\n",
    "    Load the LitSearch corpus from Hugging Face\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping paper IDs to paper content\n",
    "    \"\"\"\n",
    "    print(\"Loading LitSearch corpus dataset...\")\n",
    "    try:\n",
    "        # Load the corpus\n",
    "        corpus_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "        print(f\"Loaded {len(corpus_data)} papers from LitSearch corpus\")\n",
    "        \n",
    "        # Print a sample to understand the structure\n",
    "        if len(corpus_data) > 0:\n",
    "            print(\"Corpus data fields:\", list(corpus_data[0].keys()))\n",
    "        \n",
    "        # Create a dictionary mapping paper IDs to paper content\n",
    "        corpus_dict = {}\n",
    "        paper_id_field = None\n",
    "        \n",
    "        # Determine the field containing paper IDs\n",
    "        if len(corpus_data) > 0:\n",
    "            sample = corpus_data[0]\n",
    "            if \"paper_id\" in sample:\n",
    "                paper_id_field = \"paper_id\"\n",
    "            elif \"doc_id\" in sample:\n",
    "                paper_id_field = \"doc_id\"\n",
    "            elif \"corpusid\" in sample:\n",
    "                paper_id_field = \"corpusid\"\n",
    "            else:\n",
    "                # Find a field that looks like an ID\n",
    "                for key in sample.keys():\n",
    "                    if \"id\" in key.lower():\n",
    "                        paper_id_field = key\n",
    "                        break\n",
    "        \n",
    "        if not paper_id_field:\n",
    "            raise ValueError(\"Could not determine paper ID field in corpus dataset\")\n",
    "        \n",
    "        print(f\"Using '{paper_id_field}' as paper ID field\")\n",
    "        \n",
    "        # Create the mapping\n",
    "        for item in tqdm(corpus_data):\n",
    "            paper_id = item.get(paper_id_field)\n",
    "            if paper_id:\n",
    "                corpus_dict[paper_id] = {\n",
    "                    'title': item.get('title', ''),\n",
    "                    'abstract': item.get('abstract', ''),\n",
    "                    'full_text': item.get('full_paper', ''),  # Get full paper text if available\n",
    "                    'authors': item.get('authors', ''),\n",
    "                    'year': item.get('year', '')\n",
    "                }\n",
    "        \n",
    "        print(f\"Created corpus dictionary with {len(corpus_dict)} papers\")\n",
    "        return corpus_dict, paper_id_field\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LitSearch corpus: {e}\")\n",
    "        print(\"Falling back to using only title and abstract from user dataset\")\n",
    "        return {}, None\n",
    "\n",
    "# Load dataset paths\n",
    "user_dataset_path = \"/Users/himansh/Desktop/ANLP/litsearch/litsearch_rag_dataset_fullpaper_500.json\"  # Replace with your path\n",
    "user_dataset = load_user_dataset(user_dataset_path, num_samples=50)  # Adjust as needed\n",
    "\n",
    "# Load LitSearch corpus\n",
    "corpus_dict, paper_id_field = load_litsearch_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Enhanced Scientific Paper Chunker\n",
    "\n",
    "class ScientificPaperChunker:\n",
    "    \"\"\"Enhanced chunker for scientific papers with context preservation\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=250):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def extract_topics(self, text, n=5):\n",
    "        \"\"\"Extract key topics from text (simplified version)\"\"\"\n",
    "        # This is a simplified approach - in production, you would use NER or topic modeling\n",
    "        common_scientific_terms = [\n",
    "            \"algorithm\", \"analysis\", \"data\", \"model\", \"method\", \"results\", \n",
    "            \"neural\", \"learning\", \"network\", \"performance\", \"accuracy\", \"prediction\",\n",
    "            \"framework\", \"system\", \"implementation\", \"architecture\"\n",
    "        ]\n",
    "        \n",
    "        # Count term frequency\n",
    "        term_counts = {}\n",
    "        for term in common_scientific_terms:\n",
    "            term_counts[term] = len(re.findall(r'\\b' + term + r'\\b', text.lower()))\n",
    "        \n",
    "        # Get top terms\n",
    "        top_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [term for term, count in top_terms[:n] if count > 0]\n",
    "    \n",
    "    def chunk_paper(self, paper_id, paper_content):\n",
    "        \"\"\"\n",
    "        Chunk a scientific paper with context preservation\n",
    "        \n",
    "        Args:\n",
    "            paper_id: ID of the paper\n",
    "            paper_content: Dictionary with paper content (title, abstract, full_text, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects\n",
    "        \"\"\"\n",
    "        title = paper_content.get('title', '')\n",
    "        abstract = paper_content.get('abstract', '')\n",
    "        full_text = paper_content.get('full_text', '')\n",
    "        authors = paper_content.get('authors', '')\n",
    "        year = paper_content.get('year', '')\n",
    "        \n",
    "        # Skip if no content\n",
    "        if not title and not abstract and not full_text:\n",
    "            return []\n",
    "        \n",
    "        # Extract topics from abstract and title\n",
    "        topics = self.extract_topics(title + \" \" + abstract)\n",
    "        topics_str = \", \".join(topics) if topics else \"scientific research\"\n",
    "        \n",
    "        # Create header template for chunks\n",
    "        header_template = f\"PAPER: \\\"{title}\\\"\\n\"\n",
    "        if authors:\n",
    "            header_template += f\"AUTHORS: {authors}\\n\"\n",
    "        if year:\n",
    "            header_template += f\"YEAR: {year}\\n\"\n",
    "        header_template += f\"TOPICS: {topics_str}\\n\"\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        # Always include abstract as a standalone chunk with rich context\n",
    "        if abstract:\n",
    "            abstract_doc = Document(\n",
    "                page_content=f\"{header_template}SECTION: Abstract\\n\\n{abstract}\",\n",
    "                metadata={\n",
    "                    'paper_id': paper_id,\n",
    "                    'title': title,\n",
    "                    'section': 'abstract',\n",
    "                    'chunk_index': 0,\n",
    "                    'topics': topics,\n",
    "                    'contains_abstract': True\n",
    "                }\n",
    "            )\n",
    "            chunks.append(abstract_doc)\n",
    "        \n",
    "        # If we have full text, chunk it\n",
    "        if full_text:\n",
    "            # Split text into sentences\n",
    "            import re\n",
    "            # Improved sentence splitting regex for scientific text\n",
    "            sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', full_text)\n",
    "            \n",
    "            # Chunk sentences\n",
    "            current_chunk = []\n",
    "            current_chunk_size = 0\n",
    "            chunk_index = 1  # Start after abstract\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                \n",
    "                sentence_size = len(sentence)\n",
    "                \n",
    "                # If this sentence would make the chunk too big, finalize current chunk\n",
    "                if current_chunk and current_chunk_size + sentence_size > self.chunk_size:\n",
    "                    # Create chunk\n",
    "                    chunk_text = \" \".join(current_chunk)\n",
    "                    \n",
    "                    # Generate metadata\n",
    "                    chunk_metadata = {\n",
    "                        'paper_id': paper_id,\n",
    "                        'title': title,\n",
    "                        'chunk_index': chunk_index,\n",
    "                        'topics': topics,\n",
    "                        'contains_abstract': False\n",
    "                    }\n",
    "                    \n",
    "                    # Add current section if we can detect it\n",
    "                    current_section = self.detect_section(chunk_text)\n",
    "                    if current_section:\n",
    "                        chunk_metadata['section'] = current_section\n",
    "                    \n",
    "                    # Create the enhanced chunk with header\n",
    "                    chunk_header = f\"{header_template}CHUNK: {chunk_index}\\n\"\n",
    "                    if current_section:\n",
    "                        chunk_header += f\"SECTION: {current_section}\\n\"\n",
    "                    chunk_header += f\"\\n\"\n",
    "                    \n",
    "                    # Create document\n",
    "                    chunk_doc = Document(\n",
    "                        page_content=chunk_header + chunk_text,\n",
    "                        metadata=chunk_metadata\n",
    "                    )\n",
    "                    chunks.append(chunk_doc)\n",
    "                    \n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_size = 0\n",
    "                    overlap_chunk = []\n",
    "                    \n",
    "                    # Create overlap with previous sentences\n",
    "                    for prev_sentence in reversed(current_chunk):\n",
    "                        if overlap_size + len(prev_sentence) <= self.chunk_overlap:\n",
    "                            overlap_chunk.insert(0, prev_sentence)\n",
    "                            overlap_size += len(prev_sentence)\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    current_chunk = overlap_chunk\n",
    "                    current_chunk_size = overlap_size\n",
    "                    chunk_index += 1\n",
    "                \n",
    "                # Add current sentence to chunk\n",
    "                current_chunk.append(sentence)\n",
    "                current_chunk_size += sentence_size\n",
    "            \n",
    "            # Add the last chunk if not empty\n",
    "            if current_chunk:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                \n",
    "                # Generate metadata\n",
    "                chunk_metadata = {\n",
    "                    'paper_id': paper_id,\n",
    "                    'title': title,\n",
    "                    'chunk_index': chunk_index,\n",
    "                    'topics': topics,\n",
    "                    'contains_abstract': False\n",
    "                }\n",
    "                \n",
    "                # Add current section if we can detect it\n",
    "                current_section = self.detect_section(chunk_text)\n",
    "                if current_section:\n",
    "                    chunk_metadata['section'] = current_section\n",
    "                \n",
    "                # Create the enhanced chunk with header\n",
    "                chunk_header = f\"{header_template}CHUNK: {chunk_index}\\n\"\n",
    "                if current_section:\n",
    "                    chunk_header += f\"SECTION: {current_section}\\n\"\n",
    "                chunk_header += f\"\\n\"\n",
    "                \n",
    "                # Create document\n",
    "                chunk_doc = Document(\n",
    "                    page_content=chunk_header + chunk_text,\n",
    "                    metadata=chunk_metadata\n",
    "                )\n",
    "                chunks.append(chunk_doc)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def detect_section(self, text):\n",
    "        \"\"\"Attempt to detect which section this chunk belongs to\"\"\"\n",
    "        # Simple rule-based approach - could be enhanced with ML\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Common section keywords\n",
    "        if \"introduction\" in text_lower[:200]:\n",
    "            return \"Introduction\"\n",
    "        elif \"method\" in text_lower[:200] or \"methodology\" in text_lower[:200]:\n",
    "            return \"Methods\"\n",
    "        elif \"result\" in text_lower[:200]:\n",
    "            return \"Results\"\n",
    "        elif \"discussion\" in text_lower[:200]:\n",
    "            return \"Discussion\"\n",
    "        elif \"conclusion\" in text_lower[:200]:\n",
    "            return \"Conclusion\"\n",
    "        elif \"reference\" in text_lower[:200] or \"bibliography\" in text_lower[:200]:\n",
    "            return \"References\"\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 unique papers in user dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:00<00:00, 208.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4201 chunks from 49 papers\n",
      "  - 49 papers with full text\n",
      "  - 0 papers with abstract only\n",
      "  - Average 85.7 chunks per paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 3. Preparing Documents for LangChain\n",
    "\n",
    "def prepare_documents(user_dataset, corpus_dict):\n",
    "    \"\"\"Process papers and generate context-enhanced chunks\"\"\"\n",
    "    # Create a set of all paper IDs in the user dataset\n",
    "    paper_ids_set = set()\n",
    "    for item in user_dataset:\n",
    "        paper_id = item.get('paper_id')\n",
    "        if paper_id:\n",
    "            paper_ids_set.add(paper_id)\n",
    "    \n",
    "    print(f\"Found {len(paper_ids_set)} unique papers in user dataset\")\n",
    "    \n",
    "    # Initialize chunker\n",
    "    chunker = ScientificPaperChunker(chunk_size=1000, chunk_overlap=250)\n",
    "    \n",
    "    # Track statistics\n",
    "    papers_with_full_text = 0\n",
    "    papers_with_abstract_only = 0\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Store all chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Process each paper\n",
    "    for paper_id in tqdm(paper_ids_set):\n",
    "        # Check if paper exists in corpus with full text\n",
    "        if paper_id in corpus_dict and corpus_dict[paper_id]['full_text']:\n",
    "            # Use full paper from corpus\n",
    "            paper_content = corpus_dict[paper_id]\n",
    "            papers_with_full_text += 1\n",
    "        else:\n",
    "            # Fallback to title and abstract from user dataset\n",
    "            paper_data = next((item for item in user_dataset if item.get('paper_id') == paper_id), None)\n",
    "            if not paper_data:\n",
    "                continue\n",
    "                \n",
    "            paper_content = {\n",
    "                'title': paper_data.get('paper_title', ''),\n",
    "                'abstract': paper_data.get('paper_abstract', ''),\n",
    "                'full_text': ''  # No full text available\n",
    "            }\n",
    "            papers_with_abstract_only += 1\n",
    "        \n",
    "        # Chunk the paper\n",
    "        paper_chunks = chunker.chunk_paper(paper_id, paper_content)\n",
    "        all_chunks.extend(paper_chunks)\n",
    "        total_chunks += len(paper_chunks)\n",
    "    \n",
    "    print(f\"Created {total_chunks} chunks from {len(paper_ids_set)} papers\")\n",
    "    print(f\"  - {papers_with_full_text} papers with full text\")\n",
    "    print(f\"  - {papers_with_abstract_only} papers with abstract only\")\n",
    "    print(f\"  - Average {total_chunks / len(paper_ids_set):.1f} chunks per paper\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Create the ground truth lookup for evaluation\n",
    "def create_ground_truth_lookup(dataset):\n",
    "    ground_truth_lookup = {}\n",
    "    for item in dataset:\n",
    "        question = item.get('conceptual_question')\n",
    "        if question:\n",
    "            ground_truth_lookup[question] = {\n",
    "                'paper_id': item.get('paper_id'),\n",
    "                'answer': item.get('ground_truth_answer')\n",
    "            }\n",
    "    return ground_truth_lookup\n",
    "\n",
    "# Process documents and create chunks\n",
    "chunks = prepare_documents(user_dataset, corpus_dict)\n",
    "ground_truth_lookup = create_ground_truth_lookup(user_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "## 4. Create Retrievers\n",
    "\n",
    "# 1. BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5  # Retrieve more chunks since we're using chunking\n",
    "\n",
    "# 2. Dense Retriever with FAISS\n",
    "# Use a scientific-specific embedding model if possible\n",
    "embedding_model = \"allenai/scibert_scivocab_uncased\"  # Alternative: \"allenai/specter2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# 3. Ensemble Retriever (combines BM25 and Dense)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, dense_retriever],\n",
    "    weights=[0.3, 0.7]  # Give higher weight to dense retrieval for scientific text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Create RAG Chains with LangChain\n",
    "\n",
    "# Define a more specific prompt for scientific questions\n",
    "prompt_template = \"\"\"You are a helpful scientific assistant with expertise in research papers. Use the following pieces of context to answer the question at the end. \n",
    "\n",
    "The context contains information from scientific papers including titles, authors, and content. Use this information to provide a comprehensive and accurate answer.\n",
    "\n",
    "If you don't know the answer based on the given context, just say that you don't have enough information, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(temperature=0, \n",
    "                 model_name=\"gpt-4o\", \n",
    "                 base_url=\"https://cmu.litellm.ai\")\n",
    "\n",
    "# Create RetrievalQA chains for each retriever\n",
    "def create_qa_chain(retriever, llm=llm, prompt=PROMPT):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "# Create the chains\n",
    "bm25_chain = create_qa_chain(bm25_retriever)\n",
    "dense_chain = create_qa_chain(dense_retriever)\n",
    "ensemble_chain = create_qa_chain(ensemble_retriever)\n",
    "\n",
    "# Zero-shot chain (no retrieval)\n",
    "zero_shot_template = \"\"\"You are a helpful scientific assistant with expertise in machine learning, AI, and computer science research. Answer the following question based on your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "ZERO_SHOT_PROMPT = PromptTemplate(\n",
    "    template=zero_shot_template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Run Experiments with Enhanced Evaluation\n",
    "\n",
    "def evaluate_retrieval(retrieved_docs, ground_truth_paper_id):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of retrieved documents\n",
    "        ground_truth_paper_id: ID of the ground truth paper\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with retrieval metrics\n",
    "    \"\"\"\n",
    "    # Extract paper IDs from chunks\n",
    "    retrieved_paper_ids = set()\n",
    "    for doc in retrieved_docs:\n",
    "        paper_id = doc.metadata.get('paper_id')\n",
    "        if paper_id:\n",
    "            retrieved_paper_ids.add(paper_id)\n",
    "    \n",
    "    # Convert to list for indexing\n",
    "    retrieved_paper_ids_list = list(retrieved_paper_ids)\n",
    "    \n",
    "    # Check if ground truth is in retrieved docs\n",
    "    found = ground_truth_paper_id in retrieved_paper_ids_list\n",
    "    \n",
    "    # Calculate MRR (Mean Reciprocal Rank)\n",
    "    if found:\n",
    "        rank = retrieved_paper_ids_list.index(ground_truth_paper_id)\n",
    "        mrr = 1.0 / (rank + 1)\n",
    "    else:\n",
    "        mrr = 0.0\n",
    "    \n",
    "    # Calculate precision, recall (in this case they're the same since we have 1 relevant doc)\n",
    "    precision = 1.0 if found else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"found\": found,\n",
    "        \"mrr\": mrr,\n",
    "        \"precision\": precision,\n",
    "        \"retrieved_paper_ids\": retrieved_paper_ids_list\n",
    "    }\n",
    "\n",
    "def evaluate_answer(generated, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate answer quality using ROUGE and BERTScore\n",
    "    \n",
    "    Args:\n",
    "        generated: Generated answer\n",
    "        ground_truth: Ground truth answer\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # ROUGE scores\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, generated)\n",
    "    \n",
    "    metrics = {\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"].fmeasure\n",
    "    }\n",
    "    \n",
    "    # BERTScore with SciBERT if available\n",
    "    try:\n",
    "        # Compute BERTScore\n",
    "        P, R, F1 = bert_score(\n",
    "            [generated], \n",
    "            [ground_truth], \n",
    "            model_type=\"allenai/scibert_scivocab_uncased\",\n",
    "            lang=\"en\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Add to metrics\n",
    "        metrics.update({\n",
    "            \"bertscore_precision\": P.item(),\n",
    "            \"bertscore_recall\": R.item(),\n",
    "            \"bertscore_f1\": F1.item()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: BERTScore calculation failed: {e}\")\n",
    "        print(\"Continuing without BERTScore. Install bert_score package for complete evaluation.\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_experiment(chain, question, ground_truth_info):\n",
    "    \"\"\"Run experiment with a specific chain\"\"\"\n",
    "    # Track token usage\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain({\"query\": question})\n",
    "    \n",
    "    # Extract answer and retrieved documents\n",
    "    answer = result.get(\"result\", \"\")\n",
    "    source_docs = result.get(\"source_documents\", [])\n",
    "    \n",
    "    # Evaluate retrieval if we have source documents\n",
    "    retrieval_metrics = {}\n",
    "    if source_docs:\n",
    "        retrieval_metrics = evaluate_retrieval(source_docs, ground_truth_info['paper_id'])\n",
    "    \n",
    "    # Evaluate answer\n",
    "    answer_metrics = evaluate_answer(answer, ground_truth_info['answer'])\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieval_metrics\": retrieval_metrics,\n",
    "        \"answer_metrics\": answer_metrics,\n",
    "        \"token_usage\": {\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"cost\": cb.total_cost\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_zero_shot(llm, question, ground_truth_info):\n",
    "    \"\"\"Run zero-shot experiment (no retrieval)\"\"\"\n",
    "    # Track token usage\n",
    "    with get_openai_callback() as cb:\n",
    "        # Format prompt for LLM\n",
    "        formatted_prompt = ZERO_SHOT_PROMPT.format(question=question)\n",
    "        # Use predict instead of generate\n",
    "        answer = llm.predict(formatted_prompt)\n",
    "    \n",
    "    # Evaluate answer\n",
    "    answer_metrics = evaluate_answer(answer, ground_truth_info['answer'])\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"answer_metrics\": answer_metrics,\n",
    "        \"token_usage\": {\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"cost\": cb.total_cost\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Run All Baselines\n",
    "\n",
    "def run_all_baselines(dataset, num_samples=None):\n",
    "    \"\"\"Run all baseline methods\"\"\"\n",
    "    if num_samples and num_samples < len(dataset):\n",
    "        samples = dataset[:num_samples]\n",
    "    else:\n",
    "        samples = dataset\n",
    "    \n",
    "    results = {\n",
    "        \"bm25\": [],\n",
    "        \"dense\": [],\n",
    "        \"ensemble\": [],\n",
    "        \"zero_shot\": [],\n",
    "        \"summary\": {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Running baselines on {len(samples)} samples...\")\n",
    "    for i, sample in enumerate(tqdm(samples)):\n",
    "        question = sample.get('conceptual_question')\n",
    "        # Skip if question is missing\n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        # Get ground truth info\n",
    "        ground_truth_info = ground_truth_lookup.get(question, {})\n",
    "        if not ground_truth_info:\n",
    "            print(f\"Warning: No ground truth found for question: {question[:50]}...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing question {i+1}/{len(samples)}: {question[:100]}...\")\n",
    "        \n",
    "        # Run BM25 experiment\n",
    "        print(\"Running BM25 + LLM...\")\n",
    "        bm25_result = run_experiment(bm25_chain, question, ground_truth_info)\n",
    "        results[\"bm25\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": bm25_result[\"answer\"],\n",
    "            \"retrieval_metrics\": bm25_result[\"retrieval_metrics\"],\n",
    "            \"answer_metrics\": bm25_result[\"answer_metrics\"],\n",
    "            \"token_usage\": bm25_result[\"token_usage\"]\n",
    "        })\n",
    "        \n",
    "        # Run Dense experiment\n",
    "        print(\"Running Dense + LLM...\")\n",
    "        dense_result = run_experiment(dense_chain, question, ground_truth_info)\n",
    "        results[\"dense\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": dense_result[\"answer\"],\n",
    "            \"retrieval_metrics\": dense_result[\"retrieval_metrics\"],\n",
    "            \"answer_metrics\": dense_result[\"answer_metrics\"],\n",
    "            \"token_usage\": dense_result[\"token_usage\"]\n",
    "        })\n",
    "        \n",
    "        # Run Ensemble experiment\n",
    "        print(\"Running Ensemble + LLM...\")\n",
    "        ensemble_result = run_experiment(ensemble_chain, question, ground_truth_info)\n",
    "        results[\"ensemble\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": ensemble_result[\"answer\"],\n",
    "            \"retrieval_metrics\": ensemble_result[\"retrieval_metrics\"],\n",
    "            \"answer_metrics\": ensemble_result[\"answer_metrics\"],\n",
    "            \"token_usage\": ensemble_result[\"token_usage\"]\n",
    "        })\n",
    "        \n",
    "        # Run Zero-shot experiment\n",
    "        print(\"Running Zero-shot...\")\n",
    "        zero_shot_result = run_zero_shot(llm, question, ground_truth_info)\n",
    "        results[\"zero_shot\"].append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth_info['answer'],\n",
    "            \"answer\": zero_shot_result[\"answer\"],\n",
    "            \"answer_metrics\": zero_shot_result[\"answer_metrics\"],\n",
    "            \"token_usage\": zero_shot_result[\"token_usage\"]\n",
    "        })\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    calculate_summary_metrics(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_summary_metrics(results):\n",
    "    \"\"\"Calculate summary metrics for all methods\"\"\"\n",
    "    methods = [\"bm25\", \"dense\", \"ensemble\", \"zero_shot\"]\n",
    "    summary = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        method_results = results[method]\n",
    "        \n",
    "        # Skip if no results\n",
    "        if not method_results:\n",
    "            continue\n",
    "            \n",
    "        method_summary = {\n",
    "            \"answer_metrics\": {\n",
    "                \"rouge1\": 0.0,\n",
    "                \"rouge2\": 0.0,\n",
    "                \"rougeL\": 0.0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add BERTScore if available in the first result\n",
    "        if \"bertscore_f1\" in method_results[0][\"answer_metrics\"]:\n",
    "            method_summary[\"answer_metrics\"].update({\n",
    "                \"bertscore_precision\": 0.0,\n",
    "                \"bertscore_recall\": 0.0,\n",
    "                \"bertscore_f1\": 0.0\n",
    "            })\n",
    "        \n",
    "        # Add retrieval metrics for retrieval-based methods\n",
    "        if method != \"zero_shot\":\n",
    "            method_summary[\"retrieval_metrics\"] = {\n",
    "                \"found_rate\": 0.0,\n",
    "                \"mrr\": 0.0,\n",
    "                \"precision\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Calculate answer metrics\n",
    "        for result in method_results:\n",
    "            for metric in method_summary[\"answer_metrics\"]:\n",
    "                if metric in result[\"answer_metrics\"]:\n",
    "                    method_summary[\"answer_metrics\"][metric] += result[\"answer_metrics\"][metric]\n",
    "        \n",
    "        # Calculate retrieval metrics\n",
    "        if method != \"zero_shot\":\n",
    "            for result in method_results:\n",
    "                method_summary[\"retrieval_metrics\"][\"found_rate\"] += 1 if result[\"retrieval_metrics\"].get(\"found\", False) else 0\n",
    "                method_summary[\"retrieval_metrics\"][\"mrr\"] += result[\"retrieval_metrics\"].get(\"mrr\", 0.0)\n",
    "                method_summary[\"retrieval_metrics\"][\"precision\"] += result[\"retrieval_metrics\"].get(\"precision\", 0.0)\n",
    "        \n",
    "        # Calculate averages\n",
    "        n = len(method_results)\n",
    "        for metric in method_summary[\"answer_metrics\"]:\n",
    "            method_summary[\"answer_metrics\"][metric] /= n\n",
    "        \n",
    "        if method != \"zero_shot\":\n",
    "            for metric in method_summary[\"retrieval_metrics\"]:\n",
    "                method_summary[\"retrieval_metrics\"][metric] /= n\n",
    "        \n",
    "        summary[method] = method_summary\n",
    "    \n",
    "    results[\"summary\"] = summary\n",
    "    return summary\n",
    "\n",
    "# Run all baselines with a small sample first (adjust as needed)\n",
    "num_samples = 5  # Start small for testing\n",
    "results = run_all_baselines(user_dataset, num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Analyze Results\n",
    "\n",
    "def print_summary(results):\n",
    "    \"\"\"Print summary of results\"\"\"\n",
    "    summary = results[\"summary\"]\n",
    "    \n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(\"===============\")\n",
    "    \n",
    "    methods = [\"bm25\", \"dense\", \"ensemble\", \"zero_shot\"]\n",
    "    for method in methods:\n",
    "        if method not in summary:\n",
    "            continue\n",
    "            \n",
    "        method_summary = summary[method]\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        \n",
    "        # Print retrieval metrics\n",
    "        if \"retrieval_metrics\" in method_summary:\n",
    "            print(f\"  Retrieval Success Rate: {method_summary['retrieval_metrics']['found_rate']:.4f}\")\n",
    "            print(f\"  MRR: {method_summary['retrieval_metrics']['mrr']:.4f}\")\n",
    "            print(f\"  Precision: {method_summary['retrieval_metrics']['precision']:.4f}\")\n",
    "        \n",
    "        # Print answer metrics\n",
    "        print(f\"  ROUGE-1: {method_summary['answer_metrics']['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {method_summary['answer_metrics']['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {method_summary['answer_metrics']['rougeL']:.4f}\")\n",
    "        \n",
    "        # Print BERTScore if available\n",
    "        if \"bertscore_f1\" in method_summary[\"answer_metrics\"]:\n",
    "            print(f\"  BERTScore F1: {method_summary['answer_metrics']['bertscore_f1']:.4f}\")\n",
    "\n",
    "# Print summary\n",
    "print_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Visualize Results\n",
    "\n",
    "def plot_results(results):\n",
    "    \"\"\"Plot comparison of different methods\"\"\"\n",
    "    summary = results[\"summary\"]\n",
    "    methods = [\"bm25\", \"dense\", \"ensemble\", \"zero_shot\"]\n",
    "    \n",
    "    # Filter methods that have results\n",
    "    methods = [method for method in methods if method in summary]\n",
    "    \n",
    "    # Data for plotting\n",
    "    metrics = {\n",
    "        \"ROUGE-L\": [summary[method][\"answer_metrics\"][\"rougeL\"] for method in methods],\n",
    "        \"Found Rate\": [summary[method][\"retrieval_metrics\"][\"found_rate\"] if \"retrieval_metrics\" in summary[method] else 0 for method in methods],\n",
    "        \"MRR\": [summary[method][\"retrieval_metrics\"][\"mrr\"] if \"retrieval_metrics\" in summary[method] else 0 for method in methods]\n",
    "    }\n",
    "    \n",
    "    # Add BERTScore if available\n",
    "    if \"bertscore_f1\" in summary[methods[0]][\"answer_metrics\"]:\n",
    "        metrics[\"BERTScore F1\"] = [summary[method][\"answer_metrics\"][\"bertscore_f1\"] for method in methods]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 6))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, (metric_name, metric_values) in enumerate(metrics.items()):\n",
    "        axes[i].bar(methods, metric_values)\n",
    "        axes[i].set_title(metric_name)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, v in enumerate(metric_values):\n",
    "            axes[i].text(j, v + 0.02, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_results(results)\n",
    "\n",
    "## 10. Save Results\n",
    "\n",
    "def save_results(results, output_file=\"enhanced_scientific_rag_results.json\"):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    # Convert numpy values to Python types for JSON serialization\n",
    "    def convert_for_json(obj):\n",
    "        if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_for_json(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_for_json(value) for key, value in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    converted_results = convert_for_json(results)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(converted_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Save results\n",
    "save_results(results)\n",
    "\n",
    "## 11. Run Full Experiment\n",
    "\n",
    "# Uncomment to run on more samples\n",
    "# results = run_all_baselines(user_dataset, num_samples=50)  # Adjust as needed\n",
    "# print_summary(results)\n",
    "# plot_results(results)\n",
    "# save_results(results, \"enhanced_scientific_rag_results_full.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litsearch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
